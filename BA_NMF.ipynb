{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# make compatible with Python 2 and Python 3\n",
    "from __future__ import print_function, division, absolute_import \n",
    "\n",
    "# Remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhair\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "eng_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data= pd.read_csv('C:/Users/bhair/Documents/GitHub/LyricsMatch_DataX1/data/lyrics_updated.csv')\n",
    "# We only need the lyrics text column from the data\n",
    "data=data[data['lang']=='en']\n",
    "data_lyrics = data[['cleaned_lyrics']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    oh baby how you doing you know i m gonna cut r...\n",
      "1    playin everything so easy it s like you seem s...\n",
      "2    if you search for tenderness it isn t hard to ...\n",
      "3    oh oh oh i oh oh oh i verse if i wrote a book ...\n",
      "4    party the people the people the party it s pop...\n",
      "Name: cleaned_lyrics, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#replacing the newlines with spaces\n",
    "data['cleaned_lyrics']=data['cleaned_lyrics'].replace({'\\n':' '},regex=True)\n",
    "print(data['cleaned_lyrics'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning lyrics\n",
    "def lyrics_cleaner(lyrics):\n",
    "    \n",
    "    lyrics=re.sub(\"[^a-zA-Z]\",\" \",lyrics)\n",
    "    # Tokenize into words (all lower case)\n",
    "    lyrics = lyrics.lower().split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    eng_stopwords = set(stopwords.words(\"english\"))\n",
    "    lyrics = [word for word in lyrics if not word in eng_stopwords]\n",
    "    \n",
    "    # Join the review to one sentence\n",
    "    lyrics = ' '.join(lyrics)\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_lyrics'] = data['cleaned_lyrics'].apply(lyrics_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh baby know gonna cut right chase women made like think created special purpose know special feel baby let get lost need call work cause boss real want show feel consider lucky big deal well got key heart gonna need rather open body show secrets know inside need lie big wide strong fit much tough talk like cause back got big ego huge ego love big ego much walk like cause back usually humble right choose leave could blues call arrogant call confident decide find working damn know killing legs better yet thighs matter fact smile maybe eyes boy site see kind something like big wide strong fit much tough talk like cause back got big ego huge ego love big ego much walk like cause back walk like cause back talk like cause back back back walk like cause back big wide strong fit much tough talk like cause back got big ego huge ego huge ego love big ego much walk like cause back ego big must admit got every reason feel like bitch ego strong know need beat sing piano\n"
     ]
    }
   ],
   "source": [
    "print(data['cleaned_lyrics'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    party people people party popping sitting arou...\n",
       "5    heard church bells ringing heard choir singing...\n",
       "6    another day would spend waitin right one stari...\n",
       "7    waiting waiting waiting waiting waiting waitin...\n",
       "8    verse read magazines waiting around said wait ...\n",
       "9    n n honey better sit look around cause must bu...\n",
       "Name: cleaned_lyrics, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_lyrics'].head()\n",
    "data_lyrics=data[\"cleaned_lyrics\"]\n",
    "data_lyrics[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a CountVectorizer object and a TF-IDF Vectorizer object\n",
    "vect1=CountVectorizer (analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             max_features = 5000)\n",
    "vect2 = TfidfVectorizer(max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect1.fit(data_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1_feature_names=vect1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['add', 'addict', 'addicted', 'addiction', 'address', 'admit', 'adore', 'advance', 'advantage', 'adventure', 'advice', 'afar', 'affair', 'affection', 'afford', 'afraid', 'africa', 'african', 'afternoon', 'age']\n"
     ]
    }
   ],
   "source": [
    "#bag of words \n",
    "vect2.fit(data_lyrics)\n",
    "vect2_feature_names=vect2.get_feature_names()\n",
    "print(vect2_feature_names[30:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "#checking the size of the bag of words\n",
    "print(np.array(vect2_feature_names).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['across', 'act', 'actin', 'acting', 'action', 'actions', 'acts', 'actually', 'ad', 'adam', 'add', 'addict', 'addicted', 'addiction', 'address', 'admit', 'adore', 'advance', 'advantage', 'adventure']\n",
      "['across', 'act', 'actin', 'acting', 'action', 'actions', 'acts', 'actually', 'ad', 'adam', 'add', 'addict', 'addicted', 'addiction', 'address', 'admit', 'adore', 'advance', 'advantage', 'adventure']\n"
     ]
    }
   ],
   "source": [
    "print(vect1_feature_names[20:40])\n",
    "print(vect2_feature_names[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag1 = vect1.transform(data_lyrics) \n",
    "bag2 = vect2.transform(data_lyrics) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#just trying another function call \n",
    "vect_try=vect1.fit_transform(data_lyrics)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#just trying another function call  - contd\n",
    "print(bag1.shape, vect_try.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model to generate the decomposed matrices\n",
    "clf = decomposition.NMF(n_components=25, random_state=1)\n",
    "W1 = clf.fit_transform(bag1)\n",
    "H1 = clf.components_\n",
    "W2 = clf.fit_transform(bag2)\n",
    "H2 = clf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237947, 25) (25, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(W2.shape, H2.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(H2.T[:,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words for topic #0 :\n",
      "['could', 'way', 'say']\n",
      "\n",
      "\n",
      "Top 3 words for topic #1 :\n",
      "['true', 'heart', 'love']\n",
      "\n",
      "\n",
      "Top 3 words for topic #2 :\n",
      "['niggas', 'shit', 'nigga']\n",
      "\n",
      "\n",
      "Top 3 words for topic #3 :\n",
      "['ooh', 'whoa', 'oh']\n",
      "\n",
      "\n",
      "Top 3 words for topic #4 :\n",
      "['see', 'cause', 'know']\n",
      "\n",
      "\n",
      "Top 3 words for topic #5 :\n",
      "['ooh', 'girl', 'baby']\n",
      "\n",
      "\n",
      "Top 3 words for topic #6 :\n",
      "['show', 'go', 'let']\n",
      "\n",
      "\n",
      "Top 3 words for topic #7 :\n",
      "['see', 'tell', 'want']\n",
      "\n",
      "\n",
      "Top 3 words for topic #8 :\n",
      "['de', 'da', 'la']\n",
      "\n",
      "\n",
      "Top 3 words for topic #9 :\n",
      "['make', 'tonight', 'gonna']\n",
      "\n",
      "\n",
      "Top 3 words for topic #10 :\n",
      "['right', 'ooh', 'yeah']\n",
      "\n",
      "\n",
      "Top 3 words for topic #11 :\n",
      "['heart', 'make', 'feel']\n",
      "\n",
      "\n",
      "Top 3 words for topic #12 :\n",
      "['always', 'ever', 'never']\n",
      "\n",
      "\n",
      "Top 3 words for topic #13 :\n",
      "['two', 'day', 'one']\n",
      "\n",
      "\n",
      "Top 3 words for topic #14 :\n",
      "['feels', 'girl', 'like']\n",
      "\n",
      "\n",
      "Top 3 words for topic #15 :\n",
      "['good', 'man', 'got']\n",
      "\n",
      "\n",
      "Top 3 words for topic #16 :\n",
      "['eyes', 'world', 'life']\n",
      "\n",
      "\n",
      "Top 3 words for topic #17 :\n",
      "['mind', 'long', 'time']\n",
      "\n",
      "\n",
      "Top 3 words for topic #18 :\n",
      "['home', 'back', 'come']\n",
      "\n",
      "\n",
      "Top 3 words for topic #19 :\n",
      "['someone', 'give', 'need']\n",
      "\n",
      "\n",
      "Top 3 words for topic #20 :\n",
      "['see', 'girl', 'wanna']\n",
      "\n",
      "\n",
      "Top 3 words for topic #21 :\n",
      "['day', 'take', 'away']\n",
      "\n",
      "\n",
      "Top 3 words for topic #22 :\n",
      "['back', 'gotta', 'get']\n",
      "\n",
      "\n",
      "Top 3 words for topic #23 :\n",
      "['repeat', 'verse', 'chorus']\n",
      "\n",
      "\n",
      "Top 3 words for topic #24 :\n",
      "['na', 'girl', 'hey']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Getting the top words for each topic such that the last word [-1 position] in the list is the top word\n",
    "for i,topic in enumerate(H2):\n",
    "    print(f'Top 3 words for topic #{i} :')\n",
    "    print([vect1_feature_names[i] for i in topic.argsort()[-3:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>length</th>\n",
       "      <th>lang</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265282</th>\n",
       "      <td>who-am-i-drinking-tonight</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I gotta say\\nBoy, after only just a couple of ...</td>\n",
       "      <td>gotta say boy couple dates hands outright blow...</td>\n",
       "      <td>1597</td>\n",
       "      <td>en</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265283</th>\n",
       "      <td>liar</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I helped you find her diamond ring\\nYou made m...</td>\n",
       "      <td>helped find diamond ring made try everything t...</td>\n",
       "      <td>1009</td>\n",
       "      <td>en</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265284</th>\n",
       "      <td>last-supper</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>Look at the couple in the corner booth\\nLooks ...</td>\n",
       "      <td>look couple corner booth looks lot like lookin...</td>\n",
       "      <td>1171</td>\n",
       "      <td>en</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265285</th>\n",
       "      <td>christ-alone-live-in-studio</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>When I fly off this mortal earth\\nAnd I'm meas...</td>\n",
       "      <td>fly mortal earth measured depth girth father s...</td>\n",
       "      <td>850</td>\n",
       "      <td>en</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265286</th>\n",
       "      <td>amen</td>\n",
       "      <td>2012</td>\n",
       "      <td>edens-edge</td>\n",
       "      <td>Country</td>\n",
       "      <td>I heard from a friend of a friend of a friend ...</td>\n",
       "      <td>heard friend friend friend finally got rid gir...</td>\n",
       "      <td>1532</td>\n",
       "      <td>en</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               song  year      artist    genre  \\\n",
       "265282    who-am-i-drinking-tonight  2012  edens-edge  Country   \n",
       "265283                         liar  2012  edens-edge  Country   \n",
       "265284                  last-supper  2012  edens-edge  Country   \n",
       "265285  christ-alone-live-in-studio  2012  edens-edge  Country   \n",
       "265286                         amen  2012  edens-edge  Country   \n",
       "\n",
       "                                                   lyrics  \\\n",
       "265282  I gotta say\\nBoy, after only just a couple of ...   \n",
       "265283  I helped you find her diamond ring\\nYou made m...   \n",
       "265284  Look at the couple in the corner booth\\nLooks ...   \n",
       "265285  When I fly off this mortal earth\\nAnd I'm meas...   \n",
       "265286  I heard from a friend of a friend of a friend ...   \n",
       "\n",
       "                                           cleaned_lyrics  length lang  Topic  \n",
       "265282  gotta say boy couple dates hands outright blow...    1597   en     15  \n",
       "265283  helped find diamond ring made try everything t...    1009   en     12  \n",
       "265284  look couple corner booth looks lot like lookin...    1171   en     14  \n",
       "265285  fly mortal earth measured depth girth father s...     850   en     16  \n",
       "265286  heard friend friend friend finally got rid gir...    1532   en     24  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the most weighted topic for each song and creating a new column \"Topic\" to store that in the dataframe\n",
    "data['Topic'] = W2.argmax(axis=1)\n",
    "data.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
